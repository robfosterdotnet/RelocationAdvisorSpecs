# Rachel Torres

**Title:** Senior Data Engineer
**Department:** Data Platform
**Reports To:** Director of Data Engineering
**Location:** Austin, TX (Remote)

![Rachel Torres](./headshots/rachel-torres.jpg)

---

## Role on This Project

Rachel is the **Data Architecture Specialist** for Relocation Advisor. She designed the database schema, advises on ETL pipeline design, and ensures data quality across all sources. When you have questions about data sources, transformations, or database design, Rachel is your expert.

---

## Background

Rachel specializes in building data pipelines that ingest, transform, and serve data at scale. She has deep experience with government data sources and understands the quirks of Census, BLS, and other federal datasets.

**Education:**
- MS Data Science, UT Austin
- BS Statistics, Rice University

**Certifications:**
- Databricks Certified Data Engineer Professional
- AWS Certified Database Specialty
- dbt Analytics Engineering Certification

**Career Path:**
- 2019-Present: DataFlow Solutions (Data Engineer â†’ Senior Data Engineer)
- 2016-2019: Indeed.com (Data Engineer, Job Market Analytics)
- 2014-2016: City of Austin (Data Analyst, Open Data Initiative)
- 2012-2014: US Census Bureau (Statistical Assistant)

---

## Technical Expertise

**Languages:** Python, SQL (expert), R, Scala

**Data Tools:** PostgreSQL, Snowflake, dbt, Apache Airflow, Apache Spark

**ETL/ELT:** Custom Python pipelines, Fivetran, Airbyte

**Data Quality:** Great Expectations, dbt tests, custom validation frameworks

**Government Data:** Census API, BLS API, NOAA data, FRED, BEA

---

## Professional Style

Rachel is **detail-oriented and methodical**. She thinks deeply about data quality and edge cases. Her database designs are elegant and well-documented.

**Strengths:**
- **Data modeling**: Designs schemas that scale and make sense
- **API expertise**: Knows government data APIs inside out
- **Quality obsession**: Catches data issues others miss
- **Documentation**: Her data dictionaries are works of art

**Communication style:** Thorough and precise. Uses diagrams and examples. Happy to explain concepts multiple times in different ways.

---

## What She Cares About

1. **Data quality**: Is the data accurate, complete, and consistent?
2. **Schema design**: Will this structure serve future needs?
3. **Documentation**: Can someone else understand this data?
4. **Reproducibility**: Can we re-run this pipeline reliably?

---

## How You'll Interact

- **Sprint 1**: Database schema design and review
- **Sprints 2-3**: ETL pipeline guidance and data quality checks
- **Throughout**: Questions about data sources and transformations

**Pro tip**: Rachel loves when you ask about data quality edge cases. "What happens if this field is null?" is her favorite question.

---

## Government Data Expertise

Rachel has worked extensively with federal data sources:

**Census Bureau:**
- American Community Survey (ACS)
- Decennial Census
- Variable naming conventions
- Geographic hierarchies (FIPS codes, GEOIDs)

**Bureau of Labor Statistics:**
- Local Area Unemployment Statistics (LAUS)
- Occupational Employment Statistics (OES)
- Series ID formats

**NOAA:**
- Climate Normals
- Station data
- Weather station to county mapping challenges

---

## Data Quality Philosophy

Rachel's approach to data quality:

1. **Trust but verify**: Always spot-check against authoritative sources
2. **Handle nulls explicitly**: Never assume data will be complete
3. **Document anomalies**: When you find weird data, write it down
4. **Validate at boundaries**: Check data when it enters your system
5. **Make it reproducible**: Anyone should be able to re-run your pipeline

---

## Pet Peeves

- Assuming data is clean without checking
- Ignoring null values or special codes
- Not documenting data transformations
- Hardcoding values that could change
- Designing schemas without thinking about queries

---

## Fun Facts

- Speaks three languages (English, Spanish, Portuguese)
- Runs a popular data engineering blog
- Amateur astronomer (owns a 10" telescope)
- Competitive Scrabble player
- Makes incredible tamales (family recipe)

---

## Quotes

> "Bad data in, bad decisions out. Quality matters at every step."

> "If you can't explain where a number came from, you shouldn't trust it."

> "The Census API is quirky, but once you understand it, it's incredibly powerful."

> "Always check the data dictionary. The variable name lies, the documentation doesn't."

---

## Working With Rachel

**Do:**
- Ask about data quality edge cases
- Verify your data against the source
- Document any transformations you apply
- Ask why she made specific schema decisions
- Share interesting data anomalies you find

**Don't:**
- Assume government data is always correct
- Ignore null values or skip records silently
- Change the schema without discussing it
- Forget to handle special Census codes (-666666666, etc.)
- Load data without validating it first

---

*"Data engineering is detective work. You're always asking 'where did this come from?' and 'can I trust it?' Embrace the curiosity."*
